{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af3b91f",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce005106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import svm\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate, permutation_test_score\n",
    "import statistics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241128e8",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature data\n",
    "file = None #path to file\n",
    "\n",
    "# video metadata\n",
    "file_metadeta = None #path to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7496c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.read_csv(file,header=0)\n",
    "print(df_stats.shape)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f41d9",
   "metadata": {},
   "source": [
    "get metadata and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in metadata\n",
    "df_metadata = pd.read_csv(file_metadata,header=0, sep=';')\n",
    "df_metadata.head()\n",
    "\n",
    "# get data for merging\n",
    "df_metadata_merge = df_metadata[['videoname', 'Trial_time','Genotype', 'Genotype_class','Mouseline']] \n",
    "\n",
    "# merged data with labels and trial phase\n",
    "df_data_total = df_stats.merge(df_metadata_merge, on = 'videoname', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b1a19",
   "metadata": {},
   "source": [
    "Change string variable to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c33438",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_test_phases = {'Test': 0, 'Sample1': 1, 'Sample2': 2, 'Sample3': 3}\n",
    "for index,row in df_data_total.iterrows():\n",
    "    value = row['Trial_time']\n",
    "    df_data_total.at[index,'Trial_time'] = dict_test_phases[value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c51be",
   "metadata": {},
   "source": [
    "define different label conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TG vs WT\n",
    "y_2 = df_data_total['Genotype_class'].copy()\n",
    "\n",
    "\n",
    "# for multiclass classification\n",
    "\n",
    "# 8 TG classes vs WT\n",
    "y_9 = df_data_total['Genotype'].copy()\n",
    "for i in y_9:\n",
    "    if 'WT' in i:\n",
    "        y_9 = y_9.replace(i,'WT')\n",
    "\n",
    "# 8 TG classes and 7 WT classes\n",
    "y_15 = df_data_total['Genotype'].copy()\n",
    "\n",
    "# for changing labels to numeric incase function later complains for classificiation\n",
    "# add labels + number in dict\n",
    "#dict_y2 = \n",
    "#dict_y9 = \n",
    "#dict_y15 = \n",
    "#for i in range(len(y_2)):\n",
    "#    y_2[i] = dict_y2[y_2[i]]\n",
    " #   y_9[i] = dict_y9[y_9[i]]\n",
    " #   y_15[i] = dict_y15[y_15[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44edc59",
   "metadata": {},
   "source": [
    "Create headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total header\n",
    "header_total = list(df_data_total.columns.values)\n",
    "\n",
    "columns_to_remove = ['Genotype_class', 'videoname', 'Genotype', 'Mouseline']\n",
    "for column in columns_to_remove:\n",
    "    header_total.remove(column)\n",
    "    \n",
    "# header mean columns for baseline model\n",
    "header_mean = [i for i in header_total if 'mean' in i]\n",
    "header_mean.append('Trial_time')\n",
    "\n",
    "# create new dataset without mean features\n",
    "header_total_minmean = []\n",
    "for column in header_total:\n",
    "    if 'mean' not in column:\n",
    "        header_total_minmean.append(column)\n",
    "\n",
    "# header without median and mean columns\n",
    "header_without_median = [i for i in header_total_minmean if 'median' not in i]\n",
    "\n",
    "# header without var and mean columns\n",
    "header_without_var = [i for i in header_total_minmean if 'var' not in i]\n",
    "\n",
    "# header without std and mean columns\n",
    "header_without_std = [i for i in header_total_minmean if 'std' not in i]\n",
    "\n",
    "# header only SD\n",
    "header_sd = [i for i in header_total_minmean if 'std' in i]\n",
    "header_sd.append('Trial_time')\n",
    "\n",
    "# header only variance\n",
    "header_var = [i for i in header_total_minmean if 'var' in i]\n",
    "header_var.append('Trial_time')\n",
    "\n",
    "# header only median\n",
    "header_median = [i for i in header_total_minmean if 'median' in i]\n",
    "header_median.append('Trial_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8c912",
   "metadata": {},
   "source": [
    "Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline data with mean\n",
    "x_mean = df_data_total[header_mean]\n",
    "\n",
    "# large dataset without taking columns out\n",
    "x_whole = df_data_total[header_total]\n",
    "\n",
    "# extended model median-var-sd\n",
    "x_extended = df_data_total[header_total_minmean]\n",
    "\n",
    "# data without median\n",
    "x_without_median = df_data_total[header_without_median]\n",
    "\n",
    "# data without var\n",
    "x_without_var = df_data_total[header_without_var]\n",
    "\n",
    "# data without std\n",
    "x_without_std = df_data_total[header_without_std]\n",
    "\n",
    "# data only SD\n",
    "x_sd = df_data_total[header_sd]\n",
    "\n",
    "# data only variance\n",
    "x_var = df_data_total[header_var]\n",
    "\n",
    "# data only median\n",
    "x_median = df_data_total[header_median]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee2b6b",
   "metadata": {},
   "source": [
    "Run pipeline with scaler, pca and rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0db3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for easy changing\n",
    "# baseline = x_mean\n",
    "# extended model median-var-sd = x_extended\n",
    "\n",
    "# used data -> change titles and exported figures names\n",
    "data = x_extended\n",
    "label = y_2\n",
    "\n",
    "# define lists for scores\n",
    "score_accuracy = []\n",
    "score_f1 = []\n",
    "score_precision = []\n",
    "score_recall = []\n",
    "support = []\n",
    "\n",
    "# other variables\n",
    "count = 0\n",
    "confusion_total = []\n",
    "PCA_comp = []\n",
    "small_len_pca = None\n",
    "\n",
    "# define kfold\n",
    "kf =KFold(n_splits=5, shuffle = True, random_state = 42)\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    x_train, x_test = data.loc[train_index], data.loc[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    count += 1\n",
    "    \n",
    "    #define pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()), # can be adjusted\n",
    "        ('pca', PCA(0.95,random_state = 42)),  # now includes num components needed for 95% variance, can be adjusted\n",
    "        ('rf', RandomForestClassifier(random_state = 42))])  # tune parameters if needed\n",
    "\n",
    "    # fit data and predict\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    model_predicts = pipeline.predict(x_test)\n",
    "    scores_report = classification_report(y_test, model_predicts, output_dict=True) \n",
    "    \n",
    "    # get std and mean of scores\n",
    "    score_accuracy.append(scores_report['accuracy'])\n",
    "    support.append((scores_report['TG']['support'],scores_report['WT']['support']))\n",
    "    score_f1.append(scores_report['macro avg']['f1-score'])\n",
    "    score_precision.append(scores_report['macro avg']['precision'])\n",
    "    score_recall.append(scores_report['macro avg']['recall'])\n",
    "    \n",
    "    # plot explained variance for each fold + append for overarching figure\n",
    "    explained_variance = pipeline.steps[1][1].explained_variance_ratio_\n",
    "    PCA_comp.append(explained_variance)\n",
    "    print(len(explained_variance))\n",
    "    \n",
    "    # get smallest component for cross-validation figure\n",
    "    if small_len_pca == None:\n",
    "        small_len_pca = len(explained_variance)\n",
    "    else:\n",
    "        if len(explained_variance) < small_len_pca:\n",
    "            small_len_pca = len(explained_variance)\n",
    "            \n",
    "    cumulative_variance= np.cumsum(explained_variance)\n",
    "    plt.bar(range(0,len( explained_variance)), explained_variance, alpha=0.5, align='center', label='Individual explained variance')\n",
    "    plt.step(range(0,len(cumulative_variance)), cumulative_variance, where='mid',label='Cumulative explained variance')\n",
    "    plt.title('Explained variance over components\\n extended model, fold %i ' %count)\n",
    "    plt.ylabel('Ratio explained variance')\n",
    "    plt.xlabel('Principal component index')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/extended_variance%i.png'%count) #add output path\n",
    "    plt.clf()\n",
    "\n",
    "    # save confusion matrix values for overarching results\n",
    "    confusion = confusion_matrix(y_test,model_predicts, labels = ['TG','WT'])\n",
    "    confusion_total.append(confusion)\n",
    "    \n",
    "# print scores  \n",
    "print(\"Accuracy mean:\",  sum(score_accuracy) / len(score_accuracy),\n",
    "      \"Accuracy std:\", np.std(score_accuracy),\n",
    "      \"Precision mean\", sum(score_precision) / len(score_precision),\n",
    "      \"Precision std:\", np.std(score_precision),\n",
    "      \"Recall mean:\", sum(score_recall) / len(score_recall),\n",
    "      \"Recall std:\", np.std(score_recall),\n",
    "      \"F1 mean:\", sum(score_f1) / len(score_f1),\n",
    "      \"F1 std:\", np.std(score_f1),\n",
    "      \"Support:\", support)\n",
    " \n",
    "# permutation test\n",
    "score_pipeline, permutation_scores, pvalue = permutation_test_score(pipeline, data, label, scoring=\"accuracy\", cv=kf, n_permutations=200)\n",
    "print('pvalue permutation')\n",
    "print(pvalue)\n",
    "print('score permutation')\n",
    "print(score_pipeline)\n",
    "\n",
    "# plot permutation scores + value test \n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(permutation_scores, bins=20, density=True)\n",
    "ax.axvline(score_pipeline, ls='--', color='r', label=' accuracy original')\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "_ = ax.set_ylabel(\"Test amount\")\n",
    "plt.legend(loc='best')\n",
    "plt.title('Permutation test results, extended model')\n",
    "plt.savefig('/extended_permutation_withtext.png') #add output path\n",
    "plt.clf()\n",
    "\n",
    "# show confusion matrix as heatmap\n",
    "result_confusion = sum(confusion_total) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=result_confusion,display_labels=['TG','WT'])\n",
    "disp.plot()\n",
    "plt.title('Confusion matrix, extended model')\n",
    "plt.savefig('/extended_confusion.png') #add output path\n",
    "plt.clf()\n",
    "\n",
    "# get smallest PCA components over values\n",
    "new_PCA_comp = []\n",
    "for fold in PCA_comp:\n",
    "    new_PCA_comp.append(fold[:small_len_pca])\n",
    "components_averaged = sum(new_PCA_comp)/5\n",
    "print('comp for every fold')\n",
    "print(PCA_comp)\n",
    "print('averaged')\n",
    "print(components_averaged)\n",
    "\n",
    "# save overarching component figure\n",
    "cumulative_variance_total = np.cumsum(components_averaged)\n",
    "plt.bar(range(0,len(components_averaged)),components_averaged, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cumulative_variance_total)), cumulative_variance_total, where='mid',label='Cumulative explained variance')\n",
    "plt.title('Explained variance summed over cross-validation\\n extended model')\n",
    "plt.ylabel('Ratio explained variance')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/extended_variance.png') # add output path\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ebdef",
   "metadata": {},
   "source": [
    "Run random forest alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for easy changing\n",
    "# baseline = x_mean\n",
    "# extended model median-var-sd = x_extended\n",
    "\n",
    "# used data -> change titles and exported figures names\n",
    "data = x_extended\n",
    "label = y_2\n",
    "\n",
    "# define lists for scores\n",
    "score_accuracy = []\n",
    "score_f1 = []\n",
    "score_precision = []\n",
    "score_recall = []\n",
    "support = []\n",
    "\n",
    "# other variables\n",
    "count = 0\n",
    "confusion_total = []\n",
    "PCA_comp = []\n",
    "small_len_pca = None\n",
    "\n",
    "# define kfold\n",
    "kf =KFold(n_splits=5, shuffle = True, random_state = 42)\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    x_train, x_test = data.loc[train_index], data.loc[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    count += 1\n",
    "    \n",
    "    #define pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()), # can be adjusted\n",
    "        ('rf', RandomForestClassifier(random_state = 42))]) # can adjust parameters\n",
    "\n",
    "    # fit data and predict\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    model_predicts = pipeline.predict(x_test)\n",
    "    scores_report = classification_report(y_test, model_predicts, output_dict=True) \n",
    "    \n",
    "    # get std and mean of scores\n",
    "    score_accuracy.append(scores_report['accuracy'])\n",
    "    support.append((scores_report['TG']['support'],scores_report['WT']['support']))\n",
    "    score_f1.append(scores_report['macro avg']['f1-score'])\n",
    "    score_precision.append(scores_report['macro avg']['precision'])\n",
    "    score_recall.append(scores_report['macro avg']['recall'])\n",
    "    \n",
    "    # Extract the feature importances from the best estimator\n",
    "    feature_importances = pipeline[1][1].feature_importances_\n",
    "    \n",
    "    # Create a DataFrame to hold the feature importances and their corresponding names\n",
    "    importance_df = pd.DataFrame({'feature': data.columns.values, 'importance': feature_importances})\n",
    "    \n",
    "    # Sort the DataFrame by importance\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    a = importance_df.loc[importance_df['importance'].lt(0.001), 'importance'].idxmax()\n",
    "    print('feature importances')\n",
    "    print(importance_df.head(5))\n",
    "    # Visualize the feature importances as a bar plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=importance_df.loc[:a], x='importance', y='feature', palette='viridis')\n",
    "    plt.title('Feature Importances fold %i' %count)\n",
    "    plt.savefig('/extended_importance_rf,fold%i.png' %count) #adjust path\n",
    "    plt.clf()\n",
    "\n",
    "    # Visualize the confusion matrix as a heatmap\n",
    "    confusion = confusion_matrix(y_test,model_predicts, labels = ['TG','WT'])\n",
    "    confusion_total.append(confusion)\n",
    "    \n",
    "# print scores  \n",
    "print(\"Accuracy mean:\",  sum(score_accuracy) / len(score_accuracy),\n",
    "      \"Accuracy std:\", np.std(score_accuracy),\n",
    "      \"Precision mean\", sum(score_precision) / len(score_precision),\n",
    "      \"Precision std:\", np.std(score_precision),\n",
    "      \"Recall mean:\", sum(score_recall) / len(score_recall),\n",
    "      \"Recall std:\", np.std(score_recall),\n",
    "      \"F1 mean:\", sum(score_f1) / len(score_f1),\n",
    "      \"F1 std:\", np.std(score_f1),\n",
    "      \"Support:\", support)\n",
    " \n",
    "# permutation test\n",
    "score_pipeline, permutation_scores, pvalue = permutation_test_score(pipeline, data, label, scoring=\"accuracy\", cv=kf, n_permutations=200)\n",
    "print('pvalue permutation')\n",
    "print(pvalue)\n",
    "print('score permutation')\n",
    "print(score_pipeline)\n",
    "\n",
    "# plot permutation scores + value test \n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(permutation_scores, bins=20, density=True)\n",
    "ax.axvline(score_pipeline, ls='--', color='r', label=' accuracy original')\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "_ = ax.set_ylabel(\"Test amount\")\n",
    "plt.legend(loc='best')\n",
    "plt.title('Permutation test results, extended model')\n",
    "plt.savefig('/extended_permutation_withtext.png') #adjust output path\n",
    "plt.clf()\n",
    "\n",
    "# print confusion matrix\n",
    "result_confusion = sum(confusion_total) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=result_confusion,display_labels=['TG','WT'])\n",
    "disp.plot()\n",
    "plt.title('Confusion matrix, extended model')\n",
    "plt.savefig('/extended_confusion.png') #adjust output path\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
