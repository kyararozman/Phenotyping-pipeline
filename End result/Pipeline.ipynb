{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af3b91f",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce005106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import svm\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate, permutation_test_score\n",
    "import statistics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241128e8",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature data\n",
    "file = None #path to file\n",
    "\n",
    "# video metadata\n",
    "file_metadeta = None #path to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7496c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 381)\n"
     ]
    }
   ],
   "source": [
    "df_stats = pd.read_csv(file,header=0)\n",
    "print(df_stats.shape)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f41d9",
   "metadata": {},
   "source": [
    "get metadata and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139e5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in metadata\n",
    "df_metadata = pd.read_csv(file_metadata,header=0, sep=';')\n",
    "df_metadata.head()\n",
    "\n",
    "# get data for merging\n",
    "df_metadata_merge = df_metadata[['videoname', 'Trial_time','Genotype', 'Genotype_class','Mouseline']] \n",
    "\n",
    "# merged data with labels and trial phase\n",
    "df_data_total = df_stats.merge(df_metadata_merge, on = 'videoname', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b1a19",
   "metadata": {},
   "source": [
    "Change string variable to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c33438",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_test_phases = {'Test': 0, 'Sample1': 1, 'Sample2': 2, 'Sample3': 3}\n",
    "for index,row in df_data_total.iterrows():\n",
    "    value = row['Trial_time']\n",
    "    df_data_total.at[index,'Trial_time'] = dict_test_phases[value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c51be",
   "metadata": {},
   "source": [
    "define different label conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f1b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TG vs WT\n",
    "y_2 = df_data_total['Genotype_class'].copy()\n",
    "\n",
    "\n",
    "# for multiclass classification\n",
    "\n",
    "# 8 TG classes vs WT\n",
    "y_9 = df_data_total['Genotype'].copy()\n",
    "for i in y_9:\n",
    "    if 'WT' in i:\n",
    "        y_9 = y_9.replace(i,'WT')\n",
    "\n",
    "# 8 TG classes and 7 WT classes\n",
    "y_15 = df_data_total['Genotype'].copy()\n",
    "\n",
    "# for changing labels to numeric incase function later complains for classificiation\n",
    "# add labels + number in dict\n",
    "#dict_y2 = \n",
    "#dict_y9 = \n",
    "#dict_y15 = \n",
    "#for i in range(len(y_2)):\n",
    "#    y_2[i] = dict_y2[y_2[i]]\n",
    " #   y_9[i] = dict_y9[y_9[i]]\n",
    " #   y_15[i] = dict_y15[y_15[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44edc59",
   "metadata": {},
   "source": [
    "Create headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fff4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total header\n",
    "header_total = list(df_data_total.columns.values)\n",
    "\n",
    "columns_to_remove = ['Genotype_class', 'videoname', 'Genotype', 'Mouseline']\n",
    "for column in columns_to_remove:\n",
    "    header_total.remove(column)\n",
    "    \n",
    "# header mean columns for baseline model\n",
    "header_mean = [i for i in header_total if 'mean' in i]\n",
    "header_mean.append('Trial_time')\n",
    "\n",
    "# create new dataset without mean features\n",
    "header_total_minmean = []\n",
    "for column in header_total:\n",
    "    if 'mean' not in column:\n",
    "        header_total_minmean.append(column)\n",
    "\n",
    "# header without median and mean columns\n",
    "header_without_median = [i for i in header_total_minmean if 'median' not in i]\n",
    "\n",
    "# header without var and mean columns\n",
    "header_without_var = [i for i in header_total_minmean if 'var' not in i]\n",
    "\n",
    "# header without std and mean columns\n",
    "header_without_std = [i for i in header_total_minmean if 'std' not in i]\n",
    "\n",
    "# header only SD\n",
    "header_sd = [i for i in header_total_minmean if 'std' in i]\n",
    "header_sd.append('Trial_time')\n",
    "\n",
    "# header only variance\n",
    "header_var = [i for i in header_total_minmean if 'var' in i]\n",
    "header_var.append('Trial_time')\n",
    "\n",
    "# header only median\n",
    "header_median = [i for i in header_total_minmean if 'median' in i]\n",
    "header_median.append('Trial_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8c912",
   "metadata": {},
   "source": [
    "Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f280ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline data with mean\n",
    "x_mean = df_data_total[header_mean]\n",
    "\n",
    "# large dataset without taking columns out\n",
    "x_whole = df_data_total[header_total]\n",
    "\n",
    "# extended model median-var-sd\n",
    "x_extended = df_data_total[header_total_minmean]\n",
    "\n",
    "# data without median\n",
    "x_without_median = df_data_total[header_without_median]\n",
    "\n",
    "# data without var\n",
    "x_without_var = df_data_total[header_without_var]\n",
    "\n",
    "# data without std\n",
    "x_without_std = df_data_total[header_without_std]\n",
    "\n",
    "# data only SD\n",
    "x_sd = df_data_total[header_sd]\n",
    "\n",
    "# data only variance\n",
    "x_var = df_data_total[header_var]\n",
    "\n",
    "# data only median\n",
    "x_median = df_data_total[header_median]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee2b6b",
   "metadata": {},
   "source": [
    "Run pipeline with scaler, pca and rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aef0db3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "27\n",
      "27\n",
      "29\n",
      "27\n",
      "Accuracy mean: 0.6257668177065179 Accuracy std: 0.030873811474637688 Precision mean 0.6230258778691582 Precision std: 0.02988243972164329 Recall mean: 0.6139413509817008 Recall std: 0.026707806891417826 F1 mean: 0.6122825331230612 F1 std: 0.027941604892639874 Support: [(85, 67), (78, 74), (88, 64), (80, 72), (81, 70)]\n",
      "pvalue permutation\n",
      "0.004975124378109453\n",
      "score permutation\n",
      "0.6257668177065179\n",
      "comp for every fold\n",
      "[array([0.36880685, 0.10063221, 0.08368255, 0.07736951, 0.05285658,\n",
      "       0.04681132, 0.03227904, 0.02594482, 0.02051913, 0.01868393,\n",
      "       0.01428771, 0.01349679, 0.01279772, 0.01077125, 0.00945045,\n",
      "       0.00849026, 0.00770947, 0.00708744, 0.00608705, 0.00508444,\n",
      "       0.00479097, 0.00457144, 0.00414668, 0.00395712, 0.00341298,\n",
      "       0.00323922, 0.00312176]), array([0.40124595, 0.09756278, 0.07878779, 0.07675382, 0.04427099,\n",
      "       0.03733286, 0.03180625, 0.02686694, 0.02056279, 0.01823047,\n",
      "       0.01540451, 0.01359921, 0.01223716, 0.01020684, 0.00926429,\n",
      "       0.00832311, 0.00718091, 0.00658193, 0.00563479, 0.00486551,\n",
      "       0.00429378, 0.00394723, 0.00362809, 0.00352714, 0.0030841 ,\n",
      "       0.00301406, 0.00299453]), array([0.39722078, 0.0991961 , 0.0790377 , 0.07445741, 0.04558192,\n",
      "       0.04010613, 0.0319336 , 0.0261599 , 0.02026358, 0.01752731,\n",
      "       0.01634319, 0.01343834, 0.01196275, 0.01076313, 0.00965531,\n",
      "       0.00824407, 0.00713066, 0.00692118, 0.00576723, 0.00486538,\n",
      "       0.00443537, 0.00404075, 0.00371219, 0.00338381, 0.00319398,\n",
      "       0.0031797 , 0.00292526]), array([0.36820979, 0.10563852, 0.08247712, 0.07684382, 0.05034223,\n",
      "       0.03504792, 0.02981303, 0.02756832, 0.02019311, 0.01883985,\n",
      "       0.01812078, 0.01521601, 0.01297532, 0.01122349, 0.00972337,\n",
      "       0.0086598 , 0.00755699, 0.00740565, 0.00607595, 0.00553221,\n",
      "       0.00502345, 0.00456858, 0.00429652, 0.00397831, 0.00373538,\n",
      "       0.00346553, 0.00343244, 0.00318987, 0.00300423]), array([0.39582952, 0.09523212, 0.08115535, 0.07452567, 0.04565017,\n",
      "       0.04112873, 0.03165137, 0.02627308, 0.02015704, 0.01782825,\n",
      "       0.01639575, 0.01423986, 0.01213508, 0.01096666, 0.00995201,\n",
      "       0.00805564, 0.00709932, 0.00679407, 0.00574326, 0.00489303,\n",
      "       0.00437763, 0.00414297, 0.00362035, 0.00338879, 0.00320423,\n",
      "       0.00310146, 0.00291201])]\n",
      "averaged\n",
      "[0.38626258 0.09965235 0.0810281  0.07599004 0.04774038 0.04008539\n",
      " 0.03149666 0.02656261 0.02033913 0.01822196 0.01611039 0.01399804\n",
      " 0.0124216  0.01078628 0.00960908 0.00835458 0.00733547 0.00695805\n",
      " 0.00586166 0.00504812 0.00458424 0.00425419 0.00388076 0.00364704\n",
      " 0.00332613 0.00319999 0.0030772 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for easy changing\n",
    "# baseline = x_mean\n",
    "# extended model median-var-sd = x_extended\n",
    "\n",
    "# used data -> change titles and exported figures names\n",
    "data = x_extended\n",
    "label = y_2\n",
    "\n",
    "# define lists for scores\n",
    "score_accuracy = []\n",
    "score_f1 = []\n",
    "score_precision = []\n",
    "score_recall = []\n",
    "support = []\n",
    "\n",
    "# other variables\n",
    "count = 0\n",
    "confusion_total = []\n",
    "PCA_comp = []\n",
    "small_len_pca = None\n",
    "\n",
    "# define kfold\n",
    "kf =KFold(n_splits=5, shuffle = True, random_state = 42)\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    x_train, x_test = data.loc[train_index], data.loc[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    count += 1\n",
    "    \n",
    "    #define pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()), # can be adjusted\n",
    "        ('pca', PCA(0.95,random_state = 42)),  # now includes num components needed for 95% variance, can be adjusted\n",
    "        ('rf', RandomForestClassifier(random_state = 42))])  # tune parameters if needed\n",
    "\n",
    "    # fit data and predict\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    model_predicts = pipeline.predict(x_test)\n",
    "    scores_report = classification_report(y_test, model_predicts, output_dict=True) \n",
    "    \n",
    "    # get std and mean of scores\n",
    "    score_accuracy.append(scores_report['accuracy'])\n",
    "    support.append((scores_report['TG']['support'],scores_report['WT']['support']))\n",
    "    score_f1.append(scores_report['macro avg']['f1-score'])\n",
    "    score_precision.append(scores_report['macro avg']['precision'])\n",
    "    score_recall.append(scores_report['macro avg']['recall'])\n",
    "    \n",
    "    # plot explained variance for each fold + append for overarching figure\n",
    "    explained_variance = pipeline.steps[1][1].explained_variance_ratio_\n",
    "    PCA_comp.append(explained_variance)\n",
    "    print(len(explained_variance))\n",
    "    \n",
    "    # get smallest component for cross-validation figure\n",
    "    if small_len_pca == None:\n",
    "        small_len_pca = len(explained_variance)\n",
    "    else:\n",
    "        if len(explained_variance) < small_len_pca:\n",
    "            small_len_pca = len(explained_variance)\n",
    "            \n",
    "    cumulative_variance= np.cumsum(explained_variance)\n",
    "    plt.bar(range(0,len( explained_variance)), explained_variance, alpha=0.5, align='center', label='Individual explained variance')\n",
    "    plt.step(range(0,len(cumulative_variance)), cumulative_variance, where='mid',label='Cumulative explained variance')\n",
    "    plt.title('Explained variance over components\\n extended model, fold %i ' %count)\n",
    "    plt.ylabel('Ratio explained variance')\n",
    "    plt.xlabel('Principal component index')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/extended_variance%i.png'%count) #add output path\n",
    "    plt.clf()\n",
    "\n",
    "    # save confusion matrix values for overarching results\n",
    "    confusion = confusion_matrix(y_test,model_predicts, labels = ['TG','WT'])\n",
    "    confusion_total.append(confusion)\n",
    "    \n",
    "# print scores  \n",
    "print(\"Accuracy mean:\",  sum(score_accuracy) / len(score_accuracy),\n",
    "      \"Accuracy std:\", np.std(score_accuracy),\n",
    "      \"Precision mean\", sum(score_precision) / len(score_precision),\n",
    "      \"Precision std:\", np.std(score_precision),\n",
    "      \"Recall mean:\", sum(score_recall) / len(score_recall),\n",
    "      \"Recall std:\", np.std(score_recall),\n",
    "      \"F1 mean:\", sum(score_f1) / len(score_f1),\n",
    "      \"F1 std:\", np.std(score_f1),\n",
    "      \"Support:\", support)\n",
    " \n",
    "# permutation test\n",
    "score_pipeline, permutation_scores, pvalue = permutation_test_score(pipeline, data, label, scoring=\"accuracy\", cv=kf, n_permutations=200)\n",
    "print('pvalue permutation')\n",
    "print(pvalue)\n",
    "print('score permutation')\n",
    "print(score_pipeline)\n",
    "\n",
    "# plot permutation scores + value test \n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(permutation_scores, bins=20, density=True)\n",
    "ax.axvline(score_pipeline, ls='--', color='r', label=' accuracy original')\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "_ = ax.set_ylabel(\"Test amount\")\n",
    "plt.legend(loc='best')\n",
    "plt.title('Permutation test results, extended model')\n",
    "plt.savefig('/extended_permutation_withtext.png') #add output path\n",
    "plt.clf()\n",
    "\n",
    "# show confusion matrix as heatmap\n",
    "result_confusion = sum(confusion_total) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=result_confusion,display_labels=['TG','WT'])\n",
    "disp.plot()\n",
    "plt.title('Confusion matrix, extended model')\n",
    "plt.savefig('/extended_confusion.png') #add output path\n",
    "plt.clf()\n",
    "\n",
    "# get smallest PCA components over values\n",
    "new_PCA_comp = []\n",
    "for fold in PCA_comp:\n",
    "    new_PCA_comp.append(fold[:small_len_pca])\n",
    "components_averaged = sum(new_PCA_comp)/5\n",
    "print('comp for every fold')\n",
    "print(PCA_comp)\n",
    "print('averaged')\n",
    "print(components_averaged)\n",
    "\n",
    "# save overarching component figure\n",
    "cumulative_variance_total = np.cumsum(components_averaged)\n",
    "plt.bar(range(0,len(components_averaged)),components_averaged, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cumulative_variance_total)), cumulative_variance_total, where='mid',label='Cumulative explained variance')\n",
    "plt.title('Explained variance summed over cross-validation\\n extended model')\n",
    "plt.ylabel('Ratio explained variance')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/extended_variance.png') # add output path\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ebdef",
   "metadata": {},
   "source": [
    "Run random forest alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0939a32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature importances\n",
      "                               feature  importance\n",
      "73   var_Area_nose_schoulderL_tailbase    0.055410\n",
      "263       std_Angle_earL_tailbase_earR    0.044445\n",
      "232     std_Angle_nose_earL_schoulderR    0.035740\n",
      "230     std_Angle_nose_schoulderL_earL    0.028077\n",
      "142    median_Angle_earL_earR_tailbase    0.027999\n",
      "feature importances\n",
      "                                   feature  importance\n",
      "94       std_Area_nose_schoulderR_tailbase    0.062594\n",
      "135      median_Angle_earR_earL_schoulderL    0.051127\n",
      "87           std_Area_nose_earL_schoulderR    0.042503\n",
      "93       std_Area_nose_schoulderL_tailbase    0.041360\n",
      "158  median_Angle_earR_tailbase_schoulderL    0.035239\n",
      "feature importances\n",
      "                                     feature  importance\n",
      "203             var_Angle_earL_tailbase_earR    0.062078\n",
      "71               var_Area_nose_earR_tailbase    0.053568\n",
      "231           std_Angle_earL_nose_schoulderR    0.046580\n",
      "84   var_Area_schoulderL_schoulderR_tailbase    0.046334\n",
      "211       var_Angle_earL_schoulderR_tailbase    0.044954\n",
      "feature importances\n",
      "                                    feature  importance\n",
      "5                     median_Dist_earL_earR    0.052523\n",
      "52   median_Area_nose_schoulderL_schoulderR    0.043491\n",
      "241          std_Angle_nose_earR_schoulderR    0.039375\n",
      "120       median_Angle_earR_nose_schoulderR    0.035127\n",
      "67            var_Area_nose_earL_schoulderR    0.031226\n",
      "feature importances\n",
      "                                   feature  importance\n",
      "5                    median_Dist_earL_earR    0.056200\n",
      "124        median_Angle_nose_earR_tailbase    0.054355\n",
      "265   std_Angle_earL_schoulderL_schoulderR    0.037159\n",
      "148  median_Angle_earL_schoulderL_tailbase    0.034594\n",
      "255         std_Angle_earR_earL_schoulderL    0.033792\n",
      "Accuracy mean: 0.6600383408853259 Accuracy std: 0.037282797384172006 Precision mean 0.6581933426055422 Precision std: 0.034714907354564874 Recall mean: 0.6558792340942252 Recall std: 0.034810490575834165 F1 mean: 0.655290101299529 F1 std: 0.035535959264909815 Support: [(85, 67), (78, 74), (88, 64), (80, 72), (81, 70)]\n",
      "pvalue permutation\n",
      "0.004975124378109453\n",
      "score permutation\n",
      "0.6600383408853259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for easy changing\n",
    "# baseline = x_mean\n",
    "# extended model median-var-sd = x_extended\n",
    "\n",
    "# used data -> change titles and exported figures names\n",
    "data = x_extended\n",
    "label = y_2\n",
    "\n",
    "# define lists for scores\n",
    "score_accuracy = []\n",
    "score_f1 = []\n",
    "score_precision = []\n",
    "score_recall = []\n",
    "support = []\n",
    "\n",
    "# other variables\n",
    "count = 0\n",
    "confusion_total = []\n",
    "PCA_comp = []\n",
    "small_len_pca = None\n",
    "\n",
    "# define kfold\n",
    "kf =KFold(n_splits=5, shuffle = True, random_state = 42)\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    x_train, x_test = data.loc[train_index], data.loc[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    count += 1\n",
    "    \n",
    "    #define pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()), # can be adjusted\n",
    "        ('rf', RandomForestClassifier(random_state = 42))]) # can adjust parameters\n",
    "\n",
    "    # fit data and predict\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    model_predicts = pipeline.predict(x_test)\n",
    "    scores_report = classification_report(y_test, model_predicts, output_dict=True) \n",
    "    \n",
    "    # get std and mean of scores\n",
    "    score_accuracy.append(scores_report['accuracy'])\n",
    "    support.append((scores_report['TG']['support'],scores_report['WT']['support']))\n",
    "    score_f1.append(scores_report['macro avg']['f1-score'])\n",
    "    score_precision.append(scores_report['macro avg']['precision'])\n",
    "    score_recall.append(scores_report['macro avg']['recall'])\n",
    "    \n",
    "    # Extract the feature importances from the best estimator\n",
    "    feature_importances = pipeline[1][1].feature_importances_\n",
    "    \n",
    "    # Create a DataFrame to hold the feature importances and their corresponding names\n",
    "    importance_df = pd.DataFrame({'feature': data.columns.values, 'importance': feature_importances})\n",
    "    \n",
    "    # Sort the DataFrame by importance\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    a = importance_df.loc[importance_df['importance'].lt(0.001), 'importance'].idxmax()\n",
    "    print('feature importances')\n",
    "    print(importance_df.head(5))\n",
    "    # Visualize the feature importances as a bar plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=importance_df.loc[:a], x='importance', y='feature', palette='viridis')\n",
    "    plt.title('Feature Importances fold %i' %count)\n",
    "    plt.savefig('/extended_importance_rf,fold%i.png' %count) #adjust path\n",
    "    plt.clf()\n",
    "\n",
    "    # Visualize the confusion matrix as a heatmap\n",
    "    confusion = confusion_matrix(y_test,model_predicts, labels = ['TG','WT'])\n",
    "    confusion_total.append(confusion)\n",
    "    \n",
    "# print scores  \n",
    "print(\"Accuracy mean:\",  sum(score_accuracy) / len(score_accuracy),\n",
    "      \"Accuracy std:\", np.std(score_accuracy),\n",
    "      \"Precision mean\", sum(score_precision) / len(score_precision),\n",
    "      \"Precision std:\", np.std(score_precision),\n",
    "      \"Recall mean:\", sum(score_recall) / len(score_recall),\n",
    "      \"Recall std:\", np.std(score_recall),\n",
    "      \"F1 mean:\", sum(score_f1) / len(score_f1),\n",
    "      \"F1 std:\", np.std(score_f1),\n",
    "      \"Support:\", support)\n",
    " \n",
    "# permutation test\n",
    "score_pipeline, permutation_scores, pvalue = permutation_test_score(pipeline, data, label, scoring=\"accuracy\", cv=kf, n_permutations=200)\n",
    "print('pvalue permutation')\n",
    "print(pvalue)\n",
    "print('score permutation')\n",
    "print(score_pipeline)\n",
    "\n",
    "# plot permutation scores + value test \n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(permutation_scores, bins=20, density=True)\n",
    "ax.axvline(score_pipeline, ls='--', color='r', label=' accuracy original')\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "_ = ax.set_ylabel(\"Test amount\")\n",
    "plt.legend(loc='best')\n",
    "plt.title('Permutation test results, extended model')\n",
    "plt.savefig('/extended_permutation_withtext.png') #adjust output path\n",
    "plt.clf()\n",
    "\n",
    "# print confusion matrix\n",
    "result_confusion = sum(confusion_total) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=result_confusion,display_labels=['TG','WT'])\n",
    "disp.plot()\n",
    "plt.title('Confusion matrix, extended model')\n",
    "plt.savefig('/extended_confusion.png') #adjust output path\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
